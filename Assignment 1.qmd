---
title: "Predict the President (Neural Networks)"
author: "Kenneth Ssekimpi"
student_number: "SSKKEN001"
assignment: "Assignment 1"
editor: visual
embed-resources: true
bibliography: references.bib
format: 
  html:
    toc: true
execute: 
  warning: false
  echo: false
  cache: true
metadata:
  link-citations: true
  date-format: long
  lang: en
abstract: |
  The annual State of the Nation Address (SONA) by the President of South Africa is a crucial event that outlines government policies and the nation's future vision, with speeches providing an archive of insights into governance from 1994 to the present, and our research aims to use predictive machine learning models to attribute text segments to the respective Presidents, offering insights into leadership traits and South African governance trends.
  This research outlines a comprehensive approach, including data preparation, preprocessing, various data representation methods (Bag of Words, TF-IDF, and Word Embeddings), a range of predictive models (Random Forest, Gradient Boosting, and Neural Networks), cross-validation techniques, hyperparameter tuning, training and testing strategies, and considerations for optimizing machine learning models to classify presidential speeches in the South African context.
  The experimental results showed that the choice of data representation, predictive approach, and training/testing strategy had a substantial impact on the predictive models' performance, with word embeddings outperforming Bag of Words, and the Convolutional Neural Network being the most effective model for this classification task, although both models faced difficulties with specific classes due to an imbalanced dataset.
  These findings provide insights into model strengths and weaknesses, offering a foundation for improving the classification process within the imbalanced SONA dataset, with potential enhancements including feature engineering and increased training data; the study highlights the importance of data representation, predictive approach, and training/testing methodology in machine learning success and suggests further exploration of hyperparameter fine-tuning and feature engineering for enhanced predictive accuracy, particularly when addressing imbalanced datasets.
---

```{r}
#| label: setup
#| include: false
knitr::opts_knit$set(root.dir = "C:/Users/User/OneDrive/Documents/School/2023/Masters/STA5073Z/Assignments/Assignment 1/ds4l-assignment-1/")

knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
```

```{r}
#| label: libraries
#| message: false

# Clear global environment
# rm(list=ls())

# Libraries we need
libs <- c('caret', 'dplyr', 'ggplot2', 'lubridate', 'kableExtra', 'keras', 'tm', 'quarto', 'readr', 'stringr', 'tensorflow', 'tidyr', 'tidytext', 'tfhub')

# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
  install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}

# Load libraries
invisible(lapply(libs, library, character.only = TRUE))

use_virtualenv("r-tensorflow", required = TRUE) 
```

```{r}
#| label: unzip

# Unzip for file contents and add to new directory
unzip("sona-addresses-1994-2023.zip", exdir = "data")
```

**Keywords**: bag-of-words, feed-forward, neural networks, random forest, classification, sona, gradient-boosted trees, south africa

# Introduction

The State of the Nation Address (SONA) delivered annually by the President of South Africa stands as a critical event in the nation's political landscape. The address, traditionally presented to a joint sitting of Parliament, serves as a platform for the President to provide an extensive account of the nation's status, articulate government policies and priorities, and outline a vision for the future. The speeches delivered during SONA ceremonies offer a vital archive of insights into the governance of South Africa, spanning from its triumphant post-apartheid era in 1994 to the present day [@sagov].

Of particular significance is the observation that in the years coinciding with national elections, the SONA unfolds twice: once before the election and again after it [@sagov]. This unique occurrence reflects the dynamism of South African politics and governance, encapsulating the nation's evolving concerns and priorities.

Within this intricate and nuanced context, our research endeavours to undertake a multifaceted exploration. We have been granted access to an extensive compendium of SONA speeches, sourced from the official government website, covering the years 1994 to 2023. Our overarching objective is to construct a series of predictive machine learning models, each designed to analyze a given sentence of text and, through their learned patterns, attribute it to the respective President who delivered it.

This academic pursuit embodies an investigation into the trajectory of leadership and governance in South Africa. The predictive models decode the unique traits and qualities of each President, providing insights into public discourse as well as offering a lens through which to observe the South African government's priorities, challenges, and plans over time.

# Literature Review

## Text Representation

### Bag of Words (BoW)

*Bag of Words* (BoW) is a text representation technique designed to convert unstructed text documents into a format amenable to computational analysis [@chen2020]. The fundamental principle underpinning BoW involves the construction of a specialized "vocabulary", which comprises a comprehensive set of unique words or tokens extracted from a designated text corpus. Subsequently, each document within the corpus is transmuted into a structured vector format. In this vector representation, the elements correspond to the frequency or presence of each word contained in the previously established vocabulary [@craja2020].

The BoW method thus facilitates the transformation of each sentence or document into a numerical format, thereby enabling machine learning models to engage in classification tasks effectively.

BoW is commonly used for text classification tasks, such as sentiment analysis, spam detection, and topic modeling [@craja2020]. It is a simple yet powerful technique, which is relatively easy to implement and yields impressive results.

One limitation of using a BoW approach is that it does not account for the order of words within a sentence. This is a significant drawback, as the order of words often carries a great deal of meaning. For example, the sentence "The government serves the people" conveys a very different message from "The people serve the government", despite both sentences containing the same words. This limitation is addressed by the *n-gram* approach, which is a variant of BoW that considers the order of words within a sentence. It also does not consider the context in which words are used. For example, the word "bank" can refer to a financial institution or the side of a river. This limitation is addressed by *word embeddings*, which are a more advanced text representation technique [@chen2020].

### Term Frequency-Inverse Document Frequency (TF-IDF)

*Term Frequency-Inverse Document Frequency* (TF-IDF) is a statistical data structuring technique employed to assess the significance of a term within a document concerning its occurrence across a collection of documents [@baena-garcia2011]. This model operates on the premise that words which appear frequently within one document, but occur less frequently in others carry a strong indicator of that document's subject matter or theme. In other words, TF-IDF strives to capture the essence of a document by discerning the words that make it unique, setting them apart from more common terms. This approach to text representation is particularly advantageous as it often yields superior accuracy in predictive modeling when compared to utilizing raw word frequencies [@baena-garcia2011].

In the context of our research on identifying presidential sources in the SONA dataset, TF-IDF plays a pivotal role. It enables us to transform the SONA text into a structured and meaningful representation, emphasizing the unique words that are likely to be associated with specific presidents. This technique helps us extract the distinctive linguistic fingerprint of each presidential source in the dataset.

**Mathematical Formulation**:

TF-IDF is composed of:

$$\text{TF-IDF}_{(t, d, D)} = TF_{(t, d)} \text{ x } IDF_{(t, D)}$$

where the *Term Frequency* comprises:

$$\text{TF}_{(t, d)} = \frac{\text{Number of times term } t \text{ appears in document } d}{\text{Total number of terms in document } d}$$

and *Inverse Document Frequency* comprises:

$$ IDF_{(t, D)} = \log(\frac{\text{Total number of documents in the corpus } |D|}{\text{Number of documents containing term } t + 1}) $$

This technique, coupled with suitable prediction algorithms, empowers us to distinguish presidential writing styles and discern the source of each sentence within the SONA dataset effectively [@baena-garcia2011].

### Word Embeddings

*Word embeddings* represent a groundbreaking paradigm in natural language processing, signifying numerical vector portrayals of words. These embeddings are crafted through intricate neural network training, leveraging extensive text corpora as their foundation. They function by associating words with specific meanings or usage tendencies and are strategically arranged in proximity to each other within this vector space [@tunstall2022].

In the context of our research on identifying presidential sources in the SONA dataset, word embeddings play a vital role. They transform words from mere strings of characters into rich, context-aware representations. Consider the following example: The word "economy" and "finance" are positioned closer together in the embedding space because of their semantic similarity, indicating their shared relevance in the context of financial discussions within the SONA speeches [@delip2019].

The foremost strength of word embeddings lies in their remarkable capacity to conserve and capture semantic relationships that exist between words. In doing so, they empower neural networks with a deeper understanding of textual content. This profound comprehension extends beyond the mere identification of words, enabling the network to discern the underlying meaning of the text. This is a crucial advantage over the BoW and TF-IDF approaches, which are unable to capture the semantic relationships between words [@tunstall2022].

This capability is of great importance in our research, as it allows us to uncover nuanced differences in presidential writing styles and pinpoint the unique language choices of each president. While traditional Bag of Words (BoW) and TF-IDF approaches may falter in capturing these subtle nuances, word embeddings excel in capturing the essence of language, providing us with a powerful tool for identifying presidential sources based on their distinct linguistic fingerprints.

## Predictive Approaches

### Random Forest

The Random Forest algorithm is a powerful ensemble learning technique that works by constructing a multitude of decision trees during training and then outputting the class that is the mode of the classes (classification) or the mean prediction of the individual trees (regression) [@katuwal2018]. It is particularly well-suited for our research on identifying presidential sources within the SONA dataset.

In the context of our study, Random Forest is a valuable asset. Here's how it plays a crucial role:

1.  **Overfitting Correction**: Random Forests address a common issue in machine learning, which is the tendency of individual decision trees to overfit their training data [@hastie2009]. In the case of identifying presidential sources, this correction is essential because it ensures that the model doesn't become too specific to the training data and can generalize effectively to new SONA speeches.
2.  **Robustness and Noise Handling**: While Random Forests generally outperform standalone decision trees, they are slightly less accurate than gradient boosted trees [@hastie2009]. However, they have a distinctive advantage in our research. They are more robust when it comes to noisy datasets. SONA speeches can vary significantly in style, content, and quality, and Random Forest's robustness makes it a reliable choice for handling such diversity.

**Mathematical Formulation**:

The Random Forest algorithm is composed of [@hastie2009]:

```{=tex}
\begin{align*}
&\text{Splitting Criteria:} \quad J(D, f) = \sum_{d \in D} \left( p_d \cdot H(y_{d}) - p_{l} \cdot H(y_{l}) - p_{r} \cdot H(y_{r}) \right) \\
&\text{where} \quad H(y) \text{ is the impurity measure for class } y \\
&\text{Decision Tree:} \quad f(X) = \sum_{t=1}^{T} I(x \in R_t) \cdot c_t
\end{align*}
```
### Gradient Boosting

*Gradient Boosting* is a machine learning technique that plays a pivotal role in our quest to identify presidential sources within the vast SONA dataset. It is a technique commonly used for regression and classification problems, and it produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees [@liu2017].

1.  **Stage-wise Model Building**: Gradient Boosting builds the prediction model in a stage-wise fashion, similar to other boosting methods [@zhang2015]. In our context, this means that it can adapt and evolve over time, progressively improving its ability to discern the unique writing styles of different presidents within the SONA dataset.

2.  **Optimizing Loss Function**: One of the strengths of Gradient Boosting is its ability to optimize an arbitrary differentiable loss function [@zhang2015]. In our research, this loss function serves as the metric to gauge the error between our predictions and the actual values. By fine-tuning this loss function, we can precisely measure the accuracy of our model's predictions and, in turn, its effectiveness in identifying presidential sources.

3.  **Iterative Modeling**: The algorithm uses an iterative modeling process, which is crucial for our research. It places increased weights on data points that were inaccurately classified or showed substantial prediction errors in the previous model [@zhang2015]. This iterative approach enables our model to learn from its mistakes and continuously improve its ability to identify the sources of SONA text segments.

In our context, the loss function is the metric used to measure prediction errors, with $F(x)$ representing the current model's prediction. Additionally, $\Omega(\theta)$ is the regularization term, a function of the model's parameters. This regularization term is used to prevent overfitting, ensuring that our model doesn't become too complex and can generalize effectively to new SONA speeches.

**Mathematical Formulation**:

$$\text{Loss Function: } L(\theta) = \sum_{i=1}^{n} l(y_{i}, F(x_{i}), \theta) + \Omega(\theta)$$ In this context, the term "loss function" denotes the metric employed to gauge the error between predictions and actual values, whereas $F(x_{i}), \theta)$ represents the current model's prediction. The term $\Omega(\theta)$ refers to the regularization term, which is a function of the model's parameters. The regularization term is used to prevent overfitting by penalizing the model for being too complex [@zhang2015].

$$\theta_{(t+1)} = \theta_{(t)} - \nu \nabla L(\theta_t)F(x) = \sum_{t=1}^{T} F_t(x)$$

The ultimate prediction, $F(x)$, emerges as the cumulative aggregation of predictions stemming from each successive boosting iteration. This cumulative prediction power enhances our ability to identify presidential sources, as it represents a refined and improved model generated through multiple iterations [@zhang2015].

Gradient Boosting enables us to identify presidential sources with precision and adaptability as the model evolves over time. Its capacity to optimize loss functions and learn iteratively makes it a valuable asset in our mission to discern the unique linguistic fingerprints of different presidents within the SONA dataset.

### Feed-Forward Neural Networks

A *Feed-Forward Neural Network*, often referred to as a multilayer perceptron (MLP), is a foundational architectural framework within the realm of artificial neural networks [@fath2020]. It holds a critical role in our mission to identify presidential sources in the vast SONA dataset, serving as a versatile tool for diverse machine learning and pattern recognition endeavors.

In the context of our research, the Feed-Forward Neural Network pieces together the intricate linguistic fingerprints of different presidents within the SONA speeches. Here's how it aids us in this endeavor:

1.  **Neuron Communication**: Within this network, each neuron in a given layer accepts input from neurons in the preceding layer, simulating the way our brains process information through interconnected neurons. This communication allows the model to capture the nuanced relationships between words and phrases used by different presidents [@fath2020].

2.  **Activation Functions**: The neurons within the network compute a weighted sum of their inputs and subsequently channel the outcome through an activation function. For hidden layers, Rectified Linear Units (ReLU) are typically adopted as the activation function. This choice allows the model to capture complex patterns in the SONA speeches. At the output layer, we typically employ the softmax function, perfectly suited for classification tasks, such as identifying presidential sources [@fath2020].

3.  **Training and Learning**: The training process for a Feed-Forward Neural Network involves fine-tuning the assigned weights for each connection between neurons. This fine-tuning process aims to minimize the distinction between the projected output and the actual target values. In our case, it means ensuring that our model's predictions align with the actual presidential sources within the SONA dataset. This fine-tuning is achieved through the iterative utilization of Backpropagation, alongside optimization algorithms [@fath2020].

**Mathematical Formulation**:

$$w_{ij}^{(l)} \leftarrow w_{ij}^{(l)} - \alpha \frac{\partial J}{\partial w_{ij}^{(l)}}$$

where $w_{ij}^{(l)}$ denotes the weight of the connection between neuron $i$ in layer $l$ and neuron $j$ in layer $l+1$, and $\alpha$ represents the learning rate. $\frac{\partial J}{\partial w_{ij}^{(l)}}$ denotes the partial derivative of the cost function $J$ with respect to the weight $w_{ij}^{(l)}$ \*\*[@*\*fath2020].

**Strengths of Feed-Forward Neural Networks**:

-   They excel in capturing non-linear relationships between input and output variables [@bebis1994].
-   They have the ability to learn complex relationships between input and output variables [@bebis1994].

**Weaknesses of Feed-Forward Neural Networks**:

-   They can be prone to overfitting, especially when the number of hidden layers is large [@bebis1994].
-   Training can be computationally expensive, particularly when many hidden layers are involved [@bebis1994].

**Common Uses**:

-   Feed-Forward Neural Networks are widely utilized in classification tasks, including image classification, text classification, and sentiment analysis [@despotovic2017].

### Convolutional Neural Networks

*Convolutional Neural Networks* utilize a multi-layered architecture that is particularly adept at image classification tasks, making it a powerful tool for discerning the linguistic patterns of different presidents within the text data [@oyewola2023].

**Key Components and Operation**:

-   CNNs consist of three fundamental layers: convolutional layers, pooling layers, and fully connected layers. The heart of a CNN lies in its convolutional layers, where adaptable filters (kernels) are employed to scrutinize input data, unearthing spatial features and patterns within the text. This process is akin to examining the intricate details in an image, except our 'images' are segments of text [@tomar2023].
-   Pooling layers complement this by curtailing the spatial dimensions of feature maps, effectively downsizing the data while preserving the most relevant information. This step is crucial in reducing computational complexity while retaining essential linguistic patterns [@tomar2023].
-   Finally, the fully connected layers process the extracted features, facilitating predictions and classifications [@tomar2023].

**The Convolution Operation**: The convolution operation is central to CNNs. It entails the application of a filter (kernel) to an input 'image,' resulting in the creation of a distinctive feature map. In our case, this operation scrutinizes text segments, highlighting unique linguistic patterns and stylistic nuances [@demata2022].

**Strengths of CNNs**:

-   CNNs excel at capturing spatial and temporal dependencies in images, a capability that we adapt for textual data [@grm2018].
-   They are masters at learning spatial hierarchies of features, allowing us to uncover even subtle writing style differences among presidents [@grm2018].
-   CNNs are computationally efficient because they share parameters across spatial locations. This efficiency is crucial when dealing with extensive text data [@grm2018].

**Weaknesses of CNNs**:

-   CNNs can be computationally expensive and require a significant amount of time to train. However, the efficiency of parameter sharing helps mitigate this [@grm2018].
-   They may be prone to overfitting, particularly when the training data is limited. However, this can be addressed through proper regularization [@grm2018].
-   While adept at images and text data with spatial structure, CNNs are not inherently designed to handle sequential data, such as text [@grm2018].

**Mathematical Formulation**:

```{=tex}
\begin{align*}
(I * K)(x, y) = \sum_{i} \sum_{j} I(x - i, y - j) * K(i, j)\\
\text{Max-Pooling}(I)(x, y) = \max_{i, j} I(sx + i, sy + j)
\end{align*}
```
CNNs examine text segments with precision [@akhtar2020]. Their spatial and hierarchical feature-capturing capabilities, combined with computational efficiency, make them an invaluable asset in identifying presidential sources within the SONA dataset [@tomar2023].

# Data and Methodology

This research paper outlines the systematic approach taken to data preparation and preprocessing, explore different data representations, predictive methodologies, and training/testing techniques. The primary objective is to investigate and determine the most effective strategies for the classification of presidential speeches using machine learning models.

## Data Preparation and Preprocessing

The corpus of South African speeches spanning from 1994 to 2023 was meticulously assembled from the official South African government website. A pivotal initial step in this research lies in the realm of data preprocessing, a crucial exercise in text analysis. Its primary aim is to transmute the raw, unprocessed textual data into a well-organized format that is not only amenable to in-depth analysis, but also lays the foundation for training and employing machine learning models.

The initial stages of data preprocessing encompassed several key procedures: the extraction of vital information such as the speech date and the corresponding president. Subsequently, the text underwent a series of transformations. This involved the systematic removal of special characters, numerical digits, and punctuation marks. The rationale behind this was to eliminate potential sources of noise that could impede the analysis. By simplifying and focusing the textual content, we ensured a cleaner and more effective dataset.

Further preprocessing steps were implemented to structure the data for use in training machine learning models and facilitating predictions. Tokenization was applied to extract meaningful insights from the text. In this paper, each speech was tokenized by sentence, and each sentence was assigned a unique identification number. This practice not only allowed for precise sentence tracking, but also enabled the attrition of each sentence to the respective president who spoke the sentence.

All text characters were uniformly converted to lowercase to promote consistency and case sensitivity in the analysis. Additionally, common stop words, which hold minimal semantic value, were methodically eliminated from the text. This curation of the dataset directed the focus toward content-carrying words, thereby enhancing the detection of significant themes and patterns within the SONA speeches.

The was structured to ensure compatibility with machine learning models. Given the varying number of opportunities each president had to deliver the SONA, the dataset exhibited an inherent class imbalance, with more sentences associated with presidents who held office for longer tenures. An 80/20 split was applied to both the training and test datasets, ensuring equitable representation of sentences across different presidential tenures in order to address this imbalance. From the 80%, we used 20% for validations. This balanced allocation, while yielding a slightly reduced training dataset, mitigated potential biases in model training.

## Data Representation

### *Bag of Words vs. TF-IDF vs. Word Embeddings*

In the initial stages of our research, we considered three fundamental text representation methods: *Bag of Words* (BoW), *Term Frequency-Inverse Document Frequency* (TF-IDF), and *Word Embeddings* (WE). The choice between these approaches was crucial as they have different characteristics, expanded on in the Literature Review section. We experimentally evaluated each of these methods to determine which one would be more suitable for our predictive task.

## Predictive Approaches

### *Choice of Predictive Models*

To explore various predictive methodologies, we considered multiple machine learning models, including Random Forest, Gradient Boosting, and Neural Networks. Each of these models offers distinct strengths and weaknesses for text classification tasks, and we sought to identify which one would yield the best results for our specific problem.

The ReLU activation function was applied to the hidden layer and a softmax activation function is applied to the output layer for both the feed-forward and convolutional neural networks.

## Training and Testing Strategies

### *Cross-Validation Techniques*

In the process of evaluating the training of the Random Forest model, we employed cross-validation as an integral part of our training and testing approach. We opted for a 5-fold cross-validation strategy across the hyperparameter grid that randomly controls the number of selected features at each split of the growing tree to ensure robust model assessment. We also implemented stratified sampling to ensure that each fold contained representative samples from each class of presidential speeches.

### *Hyperparameter Tuning*

For models that require hyperparameters, such as Random Forest and Gradient Boosting, we conducted hyperparameter tuning experiments. Grid search was employed to explore different hyperparameter combinations and select the optimal settings. In the case of the Gradient Boosting model, we tuned the number of estimators, the learning rate, $\nu$ (0.01, 0.0005, 0.0001), and the maximum depth of the tree (100, 200, 500). For the Random Forest model, we tuned the number of estimators, the maximum depth of the tree, and the maximum number of features.

### *Training and Testing Split*

The dataset was divided into separate training and testing sets to assess the model's generalization ability. The split ratio was 80% for training and 20% for testing, with 20% of the training data allocated for validation.

### *Other Considerations*

When deciding on our choice of optimizers, we initially used the *Adam* optimizer in our baseline neural network. Future research could look at the *RMSProp* or *Nadam* optimizers in instances where the model does not converge well. However, we found that the Adam optimizer was sufficient for our purposes.

```{r}
#| label: read_wrangle
#| warning: false

# Get a list of all text files in the directory
text_files <- list.files(path = "data", pattern = ".txt")

# Initialize an empty list to store the data
speech_data <- c()
i = 0
# Predefined number of characters in each speech
num_chars <- c(27050, 12786, 39019, 39524, 37489, 45247, 34674, 41225, 37552, 41719, 50544, 58284, 34590, 39232, 54635, 48643, 48641, 44907, 31101, 47157, 26384, 33281, 33376, 36006, 29403, 36233, 32860, 32464, 35981, 33290, 42112, 56960, 47910, 43352, 52972, 60000)

# Loop through the list of text files and read them into R
for (file in text_files) {
  i = i + 1

  # Open the file for reading
  file_handle <- file(paste("data/", file, sep = ""), "r")
  speech <- readChar(file_handle, nchars = 60000)
  # Store the speech data in the speech_data list
  speech_data[i] <- speech
  # Close the file
  close(file_handle)
}

# Create a data frame with filenames and speech data
sona <- data.frame(filename = text_files, speech = speech_data, stringsAsFactors = FALSE)

# extract year and president for each speech
sona$year <- str_sub(sona$filename, start = 1, end = 4)
sona$president <- str_remove_all(str_extract(sona$filename, "[dA-Z].*\\."), "\\.")

# Define a regular expression for text cleaning
replace_reg <- '(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\n'

# Clean the speech data
sona <-sona %>%
  mutate(speech = str_replace_all(speech, replace_reg , ' ')
         ,date = str_sub(speech, start=1, end=30)
         ,date = str_replace_all(date, "February", "02")
         ,date = str_replace_all(date, "June", "06")
         ,date = str_replace_all(date, "Feb", "02")
         ,date = str_replace_all(date, "May", "05")
         ,date = str_replace_all(date, "Jun", "06")
         ,date = str_replace_all(date, "Thursday, ","")
         ,date = str_replace_all(date, ' ', '-')        
         ,date = str_replace_all(date, "[A-z]",'')
         ,date = str_replace_all(date, '-----', '')
         ,date = str_replace_all(date, '----', '')
         ,date = str_replace_all(date, '---', '')
         ,date = str_replace_all(date, '--', '')
  )

# Manually correct the date for the 36th speech
sona$date[36] <- "09-02-2023"
sona$year[36] <- "2023"

# Convert the date column to a date format
sona$date <- dmy(sona$date)
```

```{r}
#| label: bag_of_words
#| message: false

# Select and rename columns for the bag-of-words representation
sona <- sona %>%
  select(date, speech, year, president) %>%
  rename(President = president)

# Assign labels as integers
sona$prez_encoded <- as.integer(factor(sona$President))-1

# Define a regular expression for text tokenization
unnest_reg <- "[^A-Za-z_\\d#@']"

# Tokenize the text into sentences
tidy_sentences <- sona %>%
  mutate(speech, speech = str_replace_all(speech, "’", "'")) %>%  
  mutate(speech = str_replace_all(speech, replace_reg, '')) %>%            
  unnest_tokens(sentence, speech, token = 'sentences') %>%
  filter(str_detect(sentence, '[a-z]')) %>%
  mutate(sentID = row_number())

# Tokenize sentences into words
tidy_words <- tidy_sentences %>%
  mutate(sentence, sentence = str_replace_all(sentence, "’", "'")) %>%  
  mutate(sentence = str_replace_all(sentence, replace_reg, '')) %>%         
  unnest_tokens(word, sentence, token = 'words') %>%
  filter(str_detect(word, '[a-z]')) %>% 
  filter(!word %in% stop_words$word)

# Reproducibility
set.seed(1991)

# Create a bag-of-words representation
speech_word_bag <- tidy_words %>%
  count(word) %>%
  top_n(200, wt = n) %>%
  select(-n) # Select the top 200 words based on word frequency

# Join the bag-of-words with the text data
speech_tdf <- tidy_words %>%
  inner_join(speech_word_bag) %>%
  group_by(sentID, President, word) %>%
  count() %>%
  mutate(total = sum(n)) %>%
  ungroup()

# Create a wide format bag-of-words data frame
bag_of_words <- speech_tdf %>% 
  select(sentID, President, word, n) %>% 
  pivot_wider(names_from = word, values_from = n, values_fill = 0, names_repair = "unique")
```

# Results

## Experimental Results

### *Performance Metrics*

To evaluate the performance of each combination of data representation, predictive approach, and training/testing strategy, we employed standard classification performance metrics, such as accuracy, precision, recall, and F1-score. These metrics provided insights into the model's ability to correctly classify presidential speeches based on their authors.

1.  **Feed Forward Neural Network** (*Bag-of-Words*):\
    **Loss**: The loss, indicated as 1.3170, represents the measure of dissimilarity between the model's predictions and the actual target values \[\@mathieu2015\]. In this context, lower values are favorable, suggesting that the model is adept at making more accurate predictions.\
    **Accuracy**: The accuracy, noted as 0.4212, specifies the proportion of correct predictions made by the model [@james2017]. In this case, the model's performance appears to be around 42.12%, implying that it correctly classifies the data in roughly 42.12% of cases.

2.  **Feed-Forward Neural Network** (*Word Embeddings*):\
    **Loss**: The loss, marked as 1.6867, showcases a slightly higher value compared to the Bag-of-Words model. This implies that the model using word embeddings has somewhat more dissimilarity between its predictions and the actual values.\
    **Accuracy**: The accuracy, denoted as 0.4962, demonstrates an improved performance, with the model correctly classifying the data in approximately 49.62% of cases.

3.  **Convolutional Neural Network**:\
    **Loss**: The loss, registered as 1.7796, reflects a somewhat higher value compared to the previous two models, indicating a greater divergence between predictions and actual values.\
    **Accuracy**: The accuracy, specified as 0.5073, demonstrates an improved performance over the previous two models, with the model correctly classifying the data in approximately 50.73% of cases.

In summary, the three neural network architectures exhibit varying degrees of performance on the given task. The Feed-Forward Neural Network with Word Embeddings outperformed the Bag-of-Words variant, as indicated by higher accuracy but a slightly higher loss. Meanwhile, the Convolutional Neural Network, achieved the best accuracy, but demonstrated a marginally higher loss. These results provide valuable insights into the models' performance, allowing for a comparison of their effectiveness in the specific task at hand.

```{r}
#| label: feed_fwd_NN
#| message: false

# Assign labels as integers
bag_of_words$prez_encoded <- as.integer(factor(bag_of_words$President))-1

# Reproducibility
set.seed(1991)

# Define the sample size for the training data
sample_size <- floor(0.8 * nrow(bag_of_words))

# Randomly select indices for the training data
train_indices <- sample(seq_len(nrow(bag_of_words)), size = sample_size)

# Create training and test datasets based on the selected indices
train_data <-bag_of_words[train_indices, ]
test_data <- bag_of_words[-train_indices, ]

# Create bag-of-words matrices for the training and test data
max_words <- ncol(bag_of_words)
x_train_bag<- as.matrix(train_data[, 3:(max_words-1)]) # Extract the bag-of-words columns
x_test_bag <- as.matrix(test_data[, 3:(max_words-1)])

# Encode the labels (presidents) using one-hot encoding for the training and test data
y_train_bag <- to_categorical(train_data$prez_encoded, 
                              num_classes =length(unique(bag_of_words$prez_encoded)))
y_test_bag <- to_categorical(test_data$prez_encoded,
                             num_classes =length(unique(bag_of_words$prez_encoded)))

# Reproducibility
set.seed(1991)
# Build a neural network model using the Keras library
model_bag <- keras_model_sequential()

model_bag %>%
  layer_dense(units = 8, activation = 'relu', input_shape = c(max_words-3)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = max(train_data$prez_encoded) + 1, activation = 'softmax')

# Compile the model, specifying the loss function, optimizer, and evaluation metrics
model_bag %>% 
  compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = 'accuracy'
)

# Train the model on the training data
train_history_bag <- model_bag %>% 
  fit(
    x_train_bag, y_train_bag, 
    epochs = 30, 
    validation_split = 0.2, 
    shuffle = TRUE,
    verbose = 0
  )

# Evaluate the model on the test data
results_nn_bag <- model_bag %>%
  evaluate(x_test_bag, y_test_bag, batch_size=128, verbose = 2)
```

```{r}
#| label: word_embedding
#| message: false

# Select a subset of training data based on President labels
training_ids <- bag_of_words %>% 
  group_by(President) %>% 
  slice_sample(prop = 0.7) %>% 
  ungroup() %>%
  select(sentID)

# Define the maximum number of features (words) to consider
max_features <- 10000        

# Initialize a text tokenizer
tokenizer <- text_tokenizer(num_words = max_features)

# Fit the tokenizer on the sentences from tidy_sentences
fit_text_tokenizer(tokenizer, tidy_sentences$sentence)

# Convert text sequences to numerical sequences using the tokenizer
sequences <- tokenizer$texts_to_sequences(tidy_sentences$sentence)

# Encode the labels as integers
tidy_sentences$prez_encoded <- as.integer(factor(tidy_sentences$President))

# Convert labels to integers and subtract 1
y <- as.integer(tidy_sentences$prez_encoded)-1

# Identify training rows based on the selected training IDs
training_rows <- which(tidy_sentences$sentID %in% training_ids$sentID)

# Create training datasets
train_emb <- list()
train_emb$x <- sequences[training_rows]
train_emb$y <- y[training_rows]
train_emb$y <- to_categorical(train_emb$y,  num_classes = length(unique(tidy_sentences$prez_encoded)))

# Create test datasets
test_emb <- list()
test_emb$x <-  sequences[-training_rows]
test_emb$y <-  y[-training_rows]
test_emb$y<- to_categorical(test_emb$y,  num_classes = length(unique(tidy_sentences$prez_encoded)))

# Define the maximum sequence length
maxlen <- 32 

# Pad the sequences to a fixed length
x_train_emb <- train_emb$x %>% 
  pad_sequences(maxlen = maxlen)
x_test_emb <- test_emb$x %>% 
  pad_sequences(maxlen = maxlen)

# Build a neural network model with word embeddings
model_emb <- keras_model_sequential() %>% 
  layer_embedding(max_features, output_dim = 10, input_length = maxlen) %>%
  layer_dropout(0.2) %>%
  layer_flatten() %>%
  layer_dense(32, activation = "relu") %>%
  layer_dense(units = 6, activation = "softmax")

# Compile the model, specifying loss, optimizer, and evaluation metric
model_emb %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

# Train the model on the training data
train_history_emb <- model_emb %>%
  fit(x_train_emb, train_emb$y, epochs = 10, verbose = 0)

# Evaluate the model on the test data
results_nn_emb <- model_emb %>% 
  evaluate(x_test_emb, test_emb$y, batch_size = 64, verbose = 2)
```

```{r}
#| label: cnn
#| message: false

# Create a sequential neural network model
model_cnn <- keras_model_sequential() %>% 
  # Add an embedding layer for word embeddings
  layer_embedding(max_features, output_dim = 10, input_length = maxlen) %>%
  # Apply dropout for regularization
  layer_dropout(0.2) %>%
  # Add a 1D convolutional layer with 64 filters, a kernel size of 8, and ReLU activation
  layer_conv_1d(filters = 64, kernel_size = 8, activation = "relu") %>%
  # Apply 1D max-pooling with a pool size of 2
  layer_max_pooling_1d(pool_size = 2) %>%
  # Flatten the data
  layer_flatten() %>%
  # Flatten the data
  layer_dense(32, activation = "relu") %>%
  # Output layer with 6 units and softmax activation for classification
  layer_dense(units = 6, activation = "softmax")

# Compile the model, specifying loss, optimizer, and evaluation metric
model_cnn %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

# Train the model on the training data
train_history_cnn <- model_cnn %>%
  fit(x_train_emb, train_emb$y, batch_size = 64, epochs = 10, verbose = 0)

# Evaluate the model on the test data
results_cnn <- model_cnn %>% 
  evaluate(x_test_emb, test_emb$y, batch_size = 64, verbose = 2)
```

```{r}
#| label: gbm
#| message: false

# Reproducibility
set.seed(1991)

# Convert the "prez_encoded" column in the training and test datasets to factors
train_data$prez_encoded <- as.factor(train_data$prez_encoded)
test_data$prez_encoded <- as.factor(test_data$prez_encoded)

# Define the control settings for cross-validation
ctrl_gbm <- trainControl(method = 'cv', number = 5, verboseIter = T)

# Define a grid of hyperparameters for GBM
gbm_grid <- expand.grid(n.trees = c(100),
                        interaction.depth = c(1, 2, 6),
                        shrinkage = c(0.01, 0.005, 0.001),
                        n.minobsinnode = 1)

# Define the formula for the GBM model
fmla <- as.formula("prez_encoded ~ .")
# Get the training data without the first two columns
data_to_train <- train_data[, -(1:2)]

# Perform a grid search for the GBM model
gbm_gridsearch <- caret::train(fmla, data = data_to_train, 
                        method = 'gbm', 
                        trControl = ctrl_gbm, 
                        verbose = F, 
                        tuneGrid = gbm_grid)
# The code above trains multiple GBM models with different hyperparameter values and selects the best one based on cross-validation results.

# Make predictions on the test data using the selected GBM model
gbm_pred <- predict(gbm_gridsearch, test_data[,-(1:2)])

# Create a confusion matrix for evaluating the model's performance
gbm_conf <- confusionMatrix(gbm_pred, test_data$prez_encoded)

# Save the model, predictions, and confusion matrix
gbm_metrics <- list(gbm_gridsearch, gbm_pred, gbm_conf)
```

```{r}
#| label: random_forest
#| message: false

# Reproducibility
set.seed(1991)

# Define a grid of hyperparameters for Random Forest
ctrl_rf <- trainControl(method = 'cv', number = 5, verboseIter = T)
# Grid of hyperparameter values for "mtry"
randomForest_grid <- expand.grid(mtry = c(10, 20, 30))

# Perform a grid search for the Random Forest model
randomForest_gridsearch <- caret::train(fmla, data = data_to_train, 
                        method = 'rf', 
                        trControl = ctrl_rf, 
                        verbose = F, 
                        tuneGrid = randomForest_grid)
# The code above trains multiple Random Forest models with different "mtry"
# values and selects the best one based on cross-validation results.

# Make predictions on the test data using the selected Random Forest model
rf_pred <- predict(randomForest_gridsearch, newdata = test_data[,-(1:2)]) 

# Create a confusion matrix for evaluating the model's performance
rf_conf <- confusionMatrix(rf_pred, test_data$prez_encoded)
rf_metrics <- list(randomForest_gridsearch, rf_pred, rf_conf)
```

## Model Accuracy

```{r}
#| label: tbl-model-accuracy
#| tbl-cap: Accuracy on test data in each Machine Learning model

model_accuracies<- round(c(results_nn_bag[[2]]*100, results_nn_emb[[2]]*100,
                     results_cnn[[2]]*100,
                     rf_conf$overall[["Accuracy"]]*100,
                     gbm_conf$overall[["Accuracy"]]*100
                     ),3)

model_names <- c("Feed-Forward Neural Network (Bag of Words)",
                "Feed-Forward Neural Network (Word Embeddings Words)",
                "Convolutional Neural Nework (Word Embeddings structure)",
                "Random Forest",
                "Gradient Boost Machine")

tbl_model_acc <- cbind(model_names, model_accuracies)
colnames(tbl_model_acc) <- c("Model", "Test Accuracy")

kable(tbl_model_acc, format = "markdown", escape = FALSE, align = 'lllll') %>%
  kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

A summation of the culminated of the model accuracies can be seen in @tbl-model-accuracy.

The provided results represent the prediction performance metrics of the GBM (Gradient Boosting Machine) and Random Forest models in identifying the sources of text segments in the SONA dataset, which are attributed to different South African presidents, respectively. These models were trained on the same training data as the neural networks, but with a different set of features. The Random Forest model achieved an accuracy of 45.01% on the test data, while the GBM model achieved an accuracy of 40.73%.

```{r}
#| label: tbl-classrep
#| tbl-cap: Class representation of presidents 

prez <- c("De Klerk", "Mandela", "Mbeki", "Mothlanthe", "Zuma", "Ramaphosa")
classes <- c("Class 0", "Class 1","Class 2","Class 3","Class 4","Class 5")

tbl_class_prez <- rbind(classes, prez)
rownames(tbl_class_prez) <- c("Class", "President")

kable(tbl_class_prez, format = "markdown", escape = FALSE, digits = 3, align = 'cccccc') %>%
  kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

```{r}
#| label: tbl-rf-metrics
#| tbl-cap: Random Forest Prediction Performance Metrics

kable(round(rf_metrics[[3]]$byClass[,c(1,2,5,6,7)],4), format = "markdown", escape = FALSE, digits = 3, align = 'cccccc') %>%
  kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

### Random Forest Performance Metrics:

Shown in @tbl-rf-metrics.

-   **Sensitivity** (*True Positive Rate*): Sensitivity measures the model's ability to correctly identify instances of each class [@james2017]. Notably, Class 0 has the lowest sensitivity, indicating that the model struggles to accurately classify text segments from President De Klerk.

-   **Specificity** (*True Negative Rate*): Specificity measures the model's ability to correctly reject instances that do not belong to each class [@james2017]. The model has high specificity for most classes, particularly for Class 1 (Mandela), indicating its capacity to correctly classify non-Mandela segments.

-   **Precision**: Precision is the model's ability to make correct positive predictions [@james2017]. It varies across classes, with Class 0 (de Klerk) having relatively high precision, while Class 3 (Mothlanthe) has no precision due to a lack of correct positive predictions.

-   **Recall**: Recall measures the model's ability to identify all relevant instances [@james2017]. Classes 0 and 3 have low recall, which suggests that the model struggles to identify text segments from Presidents De Klerk and Mothlanthe.

-   **F1 Score**: The F1 score is the harmonic mean of precision and recall. It reflects the model's balance between precision and recall [@james2017]. The F1 scores vary across classes, with Class 2 (Mbeki) having a relatively high F1 score, indicating a good balance between precision and recall.

```{r}
#| label: tbl-gbm-metrics
#| tbl-cap: Gradient Boosted Trees Prediction Performance Metrics

kable(round(gbm_metrics[[3]]$byClass[,c(1,2,5,6,7)],4), format = "markdown", escape = FALSE, digits = 3, align = 'cccccc') %>%
  kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

### Gradient Boosted Trees Performance Metrics:

Shown in @tbl-gbm-metrics.

-   **Sensitivity** (*True Positive Rate*): In this case, Class 2 (Mbeki) has the highest sensitivity, indicating that the Gradient Boosted Trees model is relatively better at identifying text segments from President Mbeki.

<!-- -->

-   **Specificity** (*True Negative Rate*): The model exhibits high specificity for most classes, particularly for Class 5 (Ramaphosa), showing that it can correctly classify non-Ramaphosa segments.

-   **Precision**: Precision varies across classes, with Class 4 (Zuma) having relatively high precision, while Class 0 has very low precision, indicating that the model struggles to make correct positive predictions for President De Klerk.

-   **Recall**: Recall is low for several classes, including Class 0 and Class 3. This suggests that the model has difficulty identifying text segments from Presidents De Klerk and Mothlanthe.

-   **F1 Score**: The F1 scores vary across classes, with Class 2 having a relatively high F1 score, indicating a good balance between precision and recall, similar to the Random Forest model.

**General Observations**:

-   Both models struggle with Class 0 (President De Klerk) and Class 3 (President Mothlanthe), as evidenced by low sensitivity, precision, and recall.
-   Class 2 (President Mbeki) is relatively better identified by both models, with higher sensitivity, precision, and recall.
-   The Gradient Boosted Trees model tends to have lower sensitivity and precision for most classes compared to the Random Forest model.
-   Both models exhibit difficulties in identifying text segments from President De Klerk (Class 0) and President Mothlanthe (Class 3), which suggests that the nature of the imbalanced dataset (each had one speech, with de Klerk's been markedly shorter than Mothlanthe's) may have contributed to the models' poor performance. It is well known [@ganganwar2012] that classification algorithms perform better with more balanced data.

## Conclusions and Recommendations

The experimental results demonstrated that the choice of data representation, predictive approach, and training/testing strategy significantly influenced the performance of the predictive models. Our findings revealed that the word embeddings representation outperformed the BoW representation. Additionally, Convolutional Neural Network was identified as the most effective predictive model for this task.

# Discussion & Conclusion

These findings offer valuable insights into model strengths and weaknesses and can serve as a foundation for refining the classification process in the context of identifying presidential sources within the imbalanced SONA dataset; potential enhancements include strategies like feature engineering and obtaining more extensive training data.

## Implications

Our study underscores the pivotal role of data representation, predictive approach, and training/testing methodology in the success of machine learning tasks, particularly in dealing with imbalanced data, emphasizing the need for a systematic exploration of these facets to optimize model performance.

## Future Work

Subsequent research can delve deeper into hyperparameter fine-tuning and explore the effects of feature engineering and ensemble methods, which hold the potential to further elevate predictive accuracy, especially when addressing imbalanced datasets.

In summary, this research underscores the significance of meticulous selection of problem and data-related aspects for model optimization, especially in the context of imbalanced data. By methodically experimenting with various data representations, predictive approaches, and training/testing strategies, we can make informed choices that ultimately bolster predictive accuracy and model resilience.
