---
title: "code"
author: "Levy Lambulani Banda"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
# Clear global environment
rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'lubridate', 'purrr', 'reshape2', 'stringr', 'text2vec', 'tidyr', 'tidytext', 'topicmodels', 'tm', 'wordcloud', "kableExtra", "keras", "tensorflow", "rpart", "randomForest", "gbm", "caret")
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
  install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
```

```{r, include= FALSE}
unzip("sona-addresses-1994-2023.zip", exdir = "data")
```

```{r read_wrangle}
# Get a list of all text files in the directory
text_files <- list.files(path = "data", pattern = ".txt")
# filenames <- purrr::flatten(text_files)
# Initialize an empty list to store the data
# speech_data <- list()
speech_data <- c()
i = 0
num_chars <- c(27050, 12786, 39019, 39524, 37489, 45247, 34674, 41225, 37552, 41719, 50544, 58284, 34590, 39232, 54635, 48643, 48641, 44907, 31101, 47157, 26384, 33281, 33376, 36006, 29403, 36233, 32860, 32464, 35981, 33290, 42112, 56960, 47910, 43352, 52972, 60000)
# Loop through the list of text files and read them into R
for (file in text_files) {
  i = i + 1
  # speech <- readLines(file, warn = FALSE)
  file_handle <- file(paste("data/", file, sep = ""), "r")
  speech <- readChar(file_handle, nchars = num_chars[i])
  # speech_data[[file]] <- speech
  speech_data[i] <- speech
  close(file_handle)
}

sona <- data.frame(filename = text_files, speech = speech_data, stringsAsFactors = FALSE)

# extract year and President for each speech
sona$year <- str_sub(sona$filename, start = 1, end = 4)
sona$President <- str_remove_all(str_extract(sona$filename, "[dA-Z].*\\."), "\\.")

# clean the sona dataset by adding the date and removing unnecessary text
replace_reg <- '(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\n'

sona <-sona %>%
  mutate(speech = str_replace_all(speech, replace_reg , ' ')
         ,date = str_sub(speech, start=1, end=30)
         ,date = str_replace_all(date, "February", "02")
         ,date = str_replace_all(date, "June", "06")
         ,date = str_replace_all(date, "Feb", "02")
         ,date = str_replace_all(date, "May", "05")
         ,date = str_replace_all(date, "Jun", "06")
         ,date = str_replace_all(date, "Thursday, ","")
         ,date = str_replace_all(date, ' ', '-')        
         ,date = str_replace_all(date, "[A-z]",'')
         ,date = str_replace_all(date, '-----', '')
         ,date = str_replace_all(date, '----', '')
         ,date = str_replace_all(date, '---', '')
         ,date = str_replace_all(date, '--', '')
  )

 

sona$date[36] <- "09-02-2023"
sona$year[36] <- "2023"
sona$date <- dmy(sona$date)
sona$mfumu <- as.integer(factor(sona$President))-1

```

```{r MLP Bag of words}

unnest_reg <- "[^\\w_#@']"
replace_reg <- '(https?:.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;'

tidy_sentences <- sona %>%
  mutate(speech, speech = str_replace_all(speech, "’", "'")) %>%  
  mutate(speech = str_replace_all(speech, replace_reg, '')) %>%            
  unnest_tokens(sentence, speech, token = 'sentences') %>%
  filter(str_detect(sentence, '[a-z]')) %>%
  mutate(Sentence_ID =  row_number())
  

tidy_words<- tidy_sentences %>%
  mutate(sentence, sentence = str_replace_all(sentence, "’", "'")) %>%  
  mutate(sentence = str_replace_all(sentence, replace_reg, '')) %>%            
  unnest_tokens(word, sentence, token = 'words') %>%
  filter(str_detect(word, '[a-z]')) %>% 
  filter(!word %in% stop_words$word)

set.seed(2023)
speech_word_bag<- tidy_words %>%
  count(word) %>%
  top_n(200, wt = n) %>%
  select(-n)

speech_tdf <- tidy_words %>%
  inner_join(speech_word_bag)%>%
 group_by(Sentence_ID,President, word) %>%
  count() %>%
  mutate(total = sum(n)) %>%
  ungroup()

bag_of_words <- speech_tdf %>% 
  select(Sentence_ID,President,word, n) %>% 
  pivot_wider(names_from = word, values_from = n, values_fill = 0, names_repair = "unique")

```

```{r Feed Forward NN}
#Neural Networks 
bag_of_words$mfumu <- as.integer(factor(bag_of_words$President))-1

set.seed(2023)
sample_size <- floor(0.8 * nrow(bag_of_words))
train_indices <- sample(seq_len(nrow(bag_of_words)), size = sample_size)

train_data <-bag_of_words[train_indices, ]
test_data <- bag_of_words[-train_indices, ]

# Create a bag of words matrix for the training and test data
max_words <- ncol(bag_of_words)
x_train_bag<- as.matrix(train_data[, 3:(max_words-1)]) # Columns 3 and onwards are the BoW columns
x_test_bag <- as.matrix(test_data[, 3:(max_words-1)])
y_train_bag <- to_categorical(train_data$mfumu, 
                              num_classes =length(unique(bag_of_words$mfumu)))
y_test_bag <- to_categorical(test_data$mfumu,
                             num_classes =length(unique(bag_of_words$mfumu)))

# Build a neural network model
set.seed(2023)
model <- keras_model_sequential()
model %>%
  layer_dense(units = 8, activation = 'relu', input_shape = c(max_words-3)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = max(train_data$mfumu) + 1, activation = 'softmax')

summary(model)
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = 'accuracy'
)

# Train the model
history <- model %>% fit(
  x_train_bag, y_train_bag, 
  epochs = 30, 
  validation_split = 0.2, shuffle = TRUE
)
plot(history)

# Evaluate the model
results <- model %>%
  evaluate(x_test_bag, y_test_bag, batch_size=128, verbose = 2)

```

```{r NLP word embeddings}
training_ids <- bag_of_words %>% 
  group_by(President) %>% 
  slice_sample(prop = 0.7) %>% 
  ungroup() %>%
  select(Sentence_ID)


max_features <- 10000        # choose max_features most popular words
tokenizer = text_tokenizer(num_words = max_features)
fit_text_tokenizer(tokenizer, tidy_sentences$sentence)

sequences = tokenizer$texts_to_sequences(tidy_sentences$sentence)

tidy_sentences$mfumu<- as.integer(factor(tidy_sentences$President))
y <- as.integer(tidy_sentences$mfumu)-1

training_rows <- which(tidy_sentences$Sentence_ID %in%
                         training_ids$Sentence_ID)

train <- list()
train$x <- sequences[training_rows]
train$y <- y[training_rows]
train$y<- to_categorical(train$y,  num_classes = length(unique(tidy_sentences$mfumu)))



test <- list()
test$x <-  sequences[-training_rows]
test$y <-  y[-training_rows]
test$y<- to_categorical(test$y,  num_classes = length(unique(tidy_sentences$mfumu)))

maxlen <- 32               
x_train <- train$x %>% pad_sequences(maxlen = maxlen)
x_test <- test$x %>% pad_sequences(maxlen = maxlen)

model <- keras_model_sequential() %>% 
  layer_embedding(max_features, output_dim = 10, input_length = maxlen) %>%
  layer_dropout(0.2) %>%
  layer_flatten() %>%
  layer_dense(32, activation = "relu") %>%
  layer_dense(units = 6, activation = "softmax")

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

history <- model %>%
  fit(x_train, train$y, epochs = 10, verbose = 0)
plot(history)

results <- model %>% evaluate(x_test, test$y, batch_size = 64, verbose = 2)
```


```{r CNN }
model <- keras_model_sequential() %>% 
  layer_embedding(max_features, output_dim = 10, input_length = maxlen) %>%
  layer_dropout(0.2) %>%
  layer_conv_1d(filters = 64, kernel_size = 8, activation = "relu") %>%
  layer_max_pooling_1d(pool_size = 2) %>%
  layer_flatten() %>%
  layer_dense(32, activation = "relu") %>%
  layer_dense(units = 6, activation = "softmax")

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

history <- model %>%
  fit(x_train, train$y,
    batch_size = 64, epochs = 10, verbose = 0)
plot(history)

results <- model %>% evaluate(x_test, test$y, batch_size = 64, verbose = 2)
```


```{r Random Forest}
#Bag of words
set.seed(2023)
train_data$mfumu<-as.factor(train_data$mfumu)
test_data$mfumu<- as.factor(test_data$mfumu)

# Define a grid of hyperparameters
ctrl <- trainControl(method = 'cv', number = 5, verboseIter = T)
randomForest_grid <- expand.grid(
  mtry = c(10, 20, 30)  
)

randomForest_gridsearch <- train(mfumu ~ ., data = train_data[, -(1:2)], 
                        method = 'rf', 
                        trControl = ctrl, 
                        verbose = F, 
                        tuneGrid = randomForest_grid)

rf_pred <- predict(randomForest_gridsearch, newdata = test_data[,-(1:2)]) 
save()
## caret also provides confusion matrix options:
library(caret)
rf_confusion<- confusionMatrix(rf_pred, test_data$mfumu) #First predicted, then truth

```

```{r GBM}
set.seed(2020)
ctrl <- trainControl(method = 'cv', number = 5, verboseIter = T)
gbm_grid <- expand.grid(n.trees = c(100),
                        interaction.depth = c(1, 2, 6),
                        shrinkage = c(0.01, 0.005, 0.001),
                        n.minobsinnode = 1)


gbm_gridsearch <- train(mfumu ~ ., data = train_data[, -(1:2)], 
                        method = 'gbm', 
                        trControl = ctrl, 
                        verbose = F, 
                        tuneGrid = gbm_grid)

save(gbm_gridsearch, file = "GradientBoosts.RData")
load(GradientBoosts)
plot(gbm_gridsearch)

## Prediction
gbm_pred <- predict(gbm_gridsearch,test_data[,-(1:2)])
confusionMatrix(gbm_pred, test_data$mfumu)

```
