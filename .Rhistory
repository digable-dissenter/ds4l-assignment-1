mutate(speech, speech = str_replace_all(speech, "’", "'")) %>%
mutate(speech = str_replace_all(speech, replace_reg, '')) %>%
unnest_tokens(sentence, speech, token = 'sentences') %>%
filter(str_detect(sentence, '[a-z]')) %>%
mutate(sentID = row_number())
tidy_words<- tidy_sentences %>%
mutate(sentence, sentence = str_replace_all(sentence, "’", "'")) %>%
mutate(sentence = str_replace_all(sentence, replace_reg, '')) %>%
unnest_tokens(word, sentence, token = 'words') %>%
filter(str_detect(word, '[a-z]')) %>%
filter(!word %in% stop_words$word)
set.seed(1991)
speech_word_bag<- tidy_words %>%
count(word) %>%
top_n(200, wt = n) %>%
select(-n)
speech_tdf <- tidy_words %>%
inner_join(speech_word_bag) %>%
group_by(sentID,president, word) %>%
count() %>%
mutate(total = sum(n)) %>%
ungroup()
bag_of_words <- speech_tdf %>%
select(sentID,president,word, n) %>%
pivot_wider(names_from = word, values_from = n, values_fill = 0, names_repair = "unique")
bag_of_words <- speech_tdf %>%
select(sentID, president, word, n) %>%
dplyr::pivot_wider(names_from = word, values_from = n, values_fill = 0, names_repair = "unique")
#| label: libraries
#| message: false
# Clear global environment
# rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'lubridate', 'keras', 'tm', 'quarto', 'readr', 'stringr', 'tidyr', 'tidytext', 'tfhub')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
use_virtualenv("myenv", required = TRUE)
bag_of_words <- speech_tdf %>%
select(sentID, president, word, n) %>%
pivot_wider(names_from = word, values_from = n, values_fill = 0, names_repair = "unique")
#| label: feed_fwd_NN
# Assign labels as integers
bag_of_words$prez_encoded <- as.integer(factor(bag_of_words$president))-1
bag_of_words
View(bag_of_words)
#| label: libraries
#| message: false
# Clear global environment
rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'lubridate', 'keras', 'tm', 'quarto', 'readr', 'stringr', 'tidyr', 'tidytext', 'tfhub')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
use_virtualenv("myenv", required = TRUE)
#| label: read_wrangle
#| warning: false
# Get a list of all text files in the directory
text_files <- list.files(path = "data", pattern = ".txt")
# Initialize an empty list to store the data
speech_data <- c()
i = 0
num_chars <- c(27050, 12786, 39019, 39524, 37489, 45247, 34674, 41225, 37552, 41719, 50544, 58284, 34590, 39232, 54635, 48643, 48641, 44907, 31101, 47157, 26384, 33281, 33376, 36006, 29403, 36233, 32860, 32464, 35981, 33290, 42112, 56960, 47910, 43352, 52972, 60000)
# Loop through the list of text files and read them into R
for (file in text_files) {
i = i + 1
# speech <- readLines(file, warn = FALSE)
# Open the file for reading
file_handle <- file(paste("data/", file, sep = ""), "r")
speech <- readChar(file_handle, nchars = 60000)
# speech_data[[file]] <- speech
speech_data[i] <- speech
# Close the file
close(file_handle)
}
sona <- data.frame(filename = text_files, speech = speech_data, stringsAsFactors = FALSE)
# extract year and president for each speech
sona$year <- str_sub(sona$filename, start = 1, end = 4)
sona$president <- str_remove_all(str_extract(sona$filename, "[dA-Z].*\\."), "\\.")
# clean the sona dataset by adding the date and removing unnecessary text
replace_reg <- '(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\n'
sona <-sona %>%
mutate(speech = str_replace_all(speech, replace_reg , ' ')
,date = str_sub(speech, start=1, end=30)
,date = str_replace_all(date, "February", "02")
,date = str_replace_all(date, "June", "06")
,date = str_replace_all(date, "Feb", "02")
,date = str_replace_all(date, "May", "05")
,date = str_replace_all(date, "Jun", "06")
,date = str_replace_all(date, "Thursday, ","")
,date = str_replace_all(date, ' ', '-')
,date = str_replace_all(date, "[A-z]",'')
,date = str_replace_all(date, '-----', '')
,date = str_replace_all(date, '----', '')
,date = str_replace_all(date, '---', '')
,date = str_replace_all(date, '--', '')
)
sona$date[36] <- "09-02-2023"
sona$year[36] <- "2023"
sona$date <- dmy(sona$date)
#| label: bag_of_words
sona <- sona %>%
select(date, speech, year, president)
# Assign labels as integers
sona$prez_encoded <- as.integer(factor(sona$president))-1
unnest_reg <- "[^A-Za-z_\\d#@']"
replace_reg <- '(https?:.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;'
tidy_sentences <- sona %>%
mutate(speech, speech = str_replace_all(speech, "’", "'")) %>%
mutate(speech = str_replace_all(speech, replace_reg, '')) %>%
unnest_tokens(sentence, speech, token = 'sentences') %>%
filter(str_detect(sentence, '[a-z]')) %>%
mutate(sentID = row_number())
tidy_words<- tidy_sentences %>%
mutate(sentence, sentence = str_replace_all(sentence, "’", "'")) %>%
mutate(sentence = str_replace_all(sentence, replace_reg, '')) %>%
unnest_tokens(word, sentence, token = 'words') %>%
filter(str_detect(word, '[a-z]')) %>%
filter(!word %in% stop_words$word)
set.seed(1991)
speech_word_bag<- tidy_words %>%
count(word) %>%
top_n(200, wt = n) %>%
select(-n)
speech_tdf <- tidy_words %>%
inner_join(speech_word_bag) %>%
group_by(sentID, president, word) %>%
count() %>%
mutate(total = sum(n)) %>%
ungroup()
bag_of_words <- speech_tdf %>%
select(sentID, president, word, n) %>%
pivot_wider(names_from = word, values_from = n, values_fill = 0, names_repair = "unique")
View(bag_of_words)
colnames(bag_of_words)
View(speech_tdf)
#| label: feed_fwd_NN
# Assign labels as integers
speech_tdf %>%
select(sentID, president, word, n)
#| label: feed_fwd_NN
# Assign labels as integers
speech_tdf %>%
select(sentID, president, word, n) %>%
pivot_wider(names_from = word, values_from = n, values_fill = 0, names_repair = "unique")
#| label: bag_of_words
sona <- sona %>%
select(date, speech, year, president) %>%
rename(President = president)
# Assign labels as integers
sona$prez_encoded <- as.integer(factor(sona$president))-1
#| label: bag_of_words
sona <- sona %>%
select(date, speech, year, president) %>%
rename(President = president)
#| label: libraries
#| message: false
# Clear global environment
rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'lubridate', 'keras', 'tm', 'quarto', 'readr', 'stringr', 'tidyr', 'tidytext', 'tfhub')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
use_virtualenv("myenv", required = TRUE)
#| label: read_wrangle
#| warning: false
# Get a list of all text files in the directory
text_files <- list.files(path = "data", pattern = ".txt")
# Initialize an empty list to store the data
speech_data <- c()
i = 0
num_chars <- c(27050, 12786, 39019, 39524, 37489, 45247, 34674, 41225, 37552, 41719, 50544, 58284, 34590, 39232, 54635, 48643, 48641, 44907, 31101, 47157, 26384, 33281, 33376, 36006, 29403, 36233, 32860, 32464, 35981, 33290, 42112, 56960, 47910, 43352, 52972, 60000)
# Loop through the list of text files and read them into R
for (file in text_files) {
i = i + 1
# speech <- readLines(file, warn = FALSE)
# Open the file for reading
file_handle <- file(paste("data/", file, sep = ""), "r")
speech <- readChar(file_handle, nchars = 60000)
# speech_data[[file]] <- speech
speech_data[i] <- speech
# Close the file
close(file_handle)
}
sona <- data.frame(filename = text_files, speech = speech_data, stringsAsFactors = FALSE)
# extract year and president for each speech
sona$year <- str_sub(sona$filename, start = 1, end = 4)
sona$president <- str_remove_all(str_extract(sona$filename, "[dA-Z].*\\."), "\\.")
# clean the sona dataset by adding the date and removing unnecessary text
replace_reg <- '(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\n'
sona <-sona %>%
mutate(speech = str_replace_all(speech, replace_reg , ' ')
,date = str_sub(speech, start=1, end=30)
,date = str_replace_all(date, "February", "02")
,date = str_replace_all(date, "June", "06")
,date = str_replace_all(date, "Feb", "02")
,date = str_replace_all(date, "May", "05")
,date = str_replace_all(date, "Jun", "06")
,date = str_replace_all(date, "Thursday, ","")
,date = str_replace_all(date, ' ', '-')
,date = str_replace_all(date, "[A-z]",'')
,date = str_replace_all(date, '-----', '')
,date = str_replace_all(date, '----', '')
,date = str_replace_all(date, '---', '')
,date = str_replace_all(date, '--', '')
)
sona$date[36] <- "09-02-2023"
sona$year[36] <- "2023"
sona$date <- dmy(sona$date)
#| label: bag_of_words
sona <- sona %>%
select(date, speech, year, president) %>%
rename(President = president)
# Assign labels as integers
sona$prez_encoded <- as.integer(factor(sona$President))-1
unnest_reg <- "[^A-Za-z_\\d#@']"
replace_reg <- '(https?:.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;'
tidy_sentences <- sona %>%
mutate(speech, speech = str_replace_all(speech, "’", "'")) %>%
mutate(speech = str_replace_all(speech, replace_reg, '')) %>%
unnest_tokens(sentence, speech, token = 'sentences') %>%
filter(str_detect(sentence, '[a-z]')) %>%
mutate(sentID = row_number())
tidy_words<- tidy_sentences %>%
mutate(sentence, sentence = str_replace_all(sentence, "’", "'")) %>%
mutate(sentence = str_replace_all(sentence, replace_reg, '')) %>%
unnest_tokens(word, sentence, token = 'words') %>%
filter(str_detect(word, '[a-z]')) %>%
filter(!word %in% stop_words$word)
set.seed(1991)
speech_word_bag<- tidy_words %>%
count(word) %>%
top_n(200, wt = n) %>%
select(-n)
speech_tdf <- tidy_words %>%
inner_join(speech_word_bag) %>%
group_by(sentID, President, word) %>%
count() %>%
mutate(total = sum(n)) %>%
ungroup()
bag_of_words <- speech_tdf %>%
select(sentID, President, word, n) %>%
pivot_wider(names_from = word, values_from = n, values_fill = 0, names_repair = "unique")
View(bag_of_words)
#| label: feed_fwd_NN
# Assign labels as integers
bag_of_words$prez_encoded <- as.integer(factor(bag_of_words$President))-1
#| label: feed_fwd_NN
# Assign labels as integers
bag_of_words$prez_encoded <- as.integer(factor(bag_of_words$President))-1
set.seed(1991)
sample_size <- floor(0.8 * nrow(bag_of_words))
train_indices <- sample(seq_len(nrow(bag_of_words)), size = sample_size)
train_data <-bag_of_words[train_indices, ]
test_data <- bag_of_words[-train_indices, ]
# Create a bag of words matrix for the training and test data
max_words <- ncol(bag_of_words)
x_train_bag<- as.matrix(train_data[, 3:(max_words-1)]) # Columns 3 and onwards are the BoW columns
x_test_bag <- as.matrix(test_data[, 3:(max_words-1)])
y_train_bag <- to_categorical(train_data$mfumu,
num_classes =length(unique(bag_of_words$mfumu)))
#| label: libraries
#| message: false
# Clear global environment
rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'lubridate', 'keras', 'tm', 'quarto', 'readr', 'stringr', 'tensorflow', 'tidyr', 'tidytext', 'tfhub')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
use_virtualenv("myenv", required = TRUE)
#| label: read_wrangle
#| warning: false
# Get a list of all text files in the directory
text_files <- list.files(path = "data", pattern = ".txt")
# Initialize an empty list to store the data
speech_data <- c()
i = 0
num_chars <- c(27050, 12786, 39019, 39524, 37489, 45247, 34674, 41225, 37552, 41719, 50544, 58284, 34590, 39232, 54635, 48643, 48641, 44907, 31101, 47157, 26384, 33281, 33376, 36006, 29403, 36233, 32860, 32464, 35981, 33290, 42112, 56960, 47910, 43352, 52972, 60000)
# Loop through the list of text files and read them into R
for (file in text_files) {
i = i + 1
# speech <- readLines(file, warn = FALSE)
# Open the file for reading
file_handle <- file(paste("data/", file, sep = ""), "r")
speech <- readChar(file_handle, nchars = 60000)
# speech_data[[file]] <- speech
speech_data[i] <- speech
# Close the file
close(file_handle)
}
sona <- data.frame(filename = text_files, speech = speech_data, stringsAsFactors = FALSE)
# extract year and president for each speech
sona$year <- str_sub(sona$filename, start = 1, end = 4)
sona$president <- str_remove_all(str_extract(sona$filename, "[dA-Z].*\\."), "\\.")
# clean the sona dataset by adding the date and removing unnecessary text
replace_reg <- '(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\n'
sona <-sona %>%
mutate(speech = str_replace_all(speech, replace_reg , ' ')
,date = str_sub(speech, start=1, end=30)
,date = str_replace_all(date, "February", "02")
,date = str_replace_all(date, "June", "06")
,date = str_replace_all(date, "Feb", "02")
,date = str_replace_all(date, "May", "05")
,date = str_replace_all(date, "Jun", "06")
,date = str_replace_all(date, "Thursday, ","")
,date = str_replace_all(date, ' ', '-')
,date = str_replace_all(date, "[A-z]",'')
,date = str_replace_all(date, '-----', '')
,date = str_replace_all(date, '----', '')
,date = str_replace_all(date, '---', '')
,date = str_replace_all(date, '--', '')
)
sona$date[36] <- "09-02-2023"
sona$year[36] <- "2023"
sona$date <- dmy(sona$date)
#| label: bag_of_words
sona <- sona %>%
select(date, speech, year, president) %>%
rename(President = president)
# Assign labels as integers
sona$prez_encoded <- as.integer(factor(sona$President))-1
unnest_reg <- "[^A-Za-z_\\d#@']"
replace_reg <- '(https?:.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;'
tidy_sentences <- sona %>%
mutate(speech, speech = str_replace_all(speech, "’", "'")) %>%
mutate(speech = str_replace_all(speech, replace_reg, '')) %>%
unnest_tokens(sentence, speech, token = 'sentences') %>%
filter(str_detect(sentence, '[a-z]')) %>%
mutate(sentID = row_number())
tidy_words<- tidy_sentences %>%
mutate(sentence, sentence = str_replace_all(sentence, "’", "'")) %>%
mutate(sentence = str_replace_all(sentence, replace_reg, '')) %>%
unnest_tokens(word, sentence, token = 'words') %>%
filter(str_detect(word, '[a-z]')) %>%
filter(!word %in% stop_words$word)
set.seed(1991)
speech_word_bag<- tidy_words %>%
count(word) %>%
top_n(200, wt = n) %>%
select(-n)
speech_tdf <- tidy_words %>%
inner_join(speech_word_bag) %>%
group_by(sentID, President, word) %>%
count() %>%
mutate(total = sum(n)) %>%
ungroup()
bag_of_words <- speech_tdf %>%
select(sentID, President, word, n) %>%
pivot_wider(names_from = word, values_from = n, values_fill = 0, names_repair = "unique")
#| label: feed_fwd_NN
# Assign labels as integers
bag_of_words$prez_encoded <- as.integer(factor(bag_of_words$President))-1
set.seed(1991)
sample_size <- floor(0.8 * nrow(bag_of_words))
train_indices <- sample(seq_len(nrow(bag_of_words)), size = sample_size)
train_data <-bag_of_words[train_indices, ]
test_data <- bag_of_words[-train_indices, ]
# Create a bag of words matrix for the training and test data
max_words <- ncol(bag_of_words)
x_train_bag<- as.matrix(train_data[, 3:(max_words-1)]) # Columns 3 and onwards are the BoW columns
x_test_bag <- as.matrix(test_data[, 3:(max_words-1)])
y_train_bag <- to_categorical(train_data$mfumu,
num_classes =length(unique(bag_of_words$mfumu)))
#| label: feed_fwd_NN
# Assign labels as integers
bag_of_words$prez_encoded <- as.integer(factor(bag_of_words$President))-1
set.seed(1991)
sample_size <- floor(0.8 * nrow(bag_of_words))
train_indices <- sample(seq_len(nrow(bag_of_words)), size = sample_size)
train_data <-bag_of_words[train_indices, ]
test_data <- bag_of_words[-train_indices, ]
# Create a bag of words matrix for the training and test data
max_words <- ncol(bag_of_words)
x_train_bag<- as.matrix(train_data[, 3:(max_words-1)]) # Columns 3 and onwards are the BoW columns
x_test_bag <- as.matrix(test_data[, 3:(max_words-1)])
y_train_bag <- to_categorical(train_data$prez_encoded,
num_classes =length(unique(bag_of_words$prez_encoded)))
install_tensorflow()
# Clear global environment
rm(list=ls())
use_virtualenv("myenv", required = TRUE)
install_tensorflow()
#| label: libraries
#| message: false
# Clear global environment
rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'lubridate', 'keras', 'tm', 'quarto', 'readr', 'stringr', 'tensorflow', 'tidyr', 'tidytext', 'tfhub')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
use_virtualenv("myenv", required = TRUE)
#| label: unzip
unzip("sona-addresses-1994-2023.zip", exdir = "data")
#| label: read_wrangle
#| warning: false
# Get a list of all text files in the directory
text_files <- list.files(path = "data", pattern = ".txt")
# Initialize an empty list to store the data
speech_data <- c()
i = 0
num_chars <- c(27050, 12786, 39019, 39524, 37489, 45247, 34674, 41225, 37552, 41719, 50544, 58284, 34590, 39232, 54635, 48643, 48641, 44907, 31101, 47157, 26384, 33281, 33376, 36006, 29403, 36233, 32860, 32464, 35981, 33290, 42112, 56960, 47910, 43352, 52972, 60000)
# Loop through the list of text files and read them into R
for (file in text_files) {
i = i + 1
# speech <- readLines(file, warn = FALSE)
# Open the file for reading
file_handle <- file(paste("data/", file, sep = ""), "r")
speech <- readChar(file_handle, nchars = 60000)
# speech_data[[file]] <- speech
speech_data[i] <- speech
# Close the file
close(file_handle)
}
sona <- data.frame(filename = text_files, speech = speech_data, stringsAsFactors = FALSE)
# extract year and president for each speech
sona$year <- str_sub(sona$filename, start = 1, end = 4)
sona$president <- str_remove_all(str_extract(sona$filename, "[dA-Z].*\\."), "\\.")
# clean the sona dataset by adding the date and removing unnecessary text
replace_reg <- '(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\n'
sona <-sona %>%
mutate(speech = str_replace_all(speech, replace_reg , ' ')
,date = str_sub(speech, start=1, end=30)
,date = str_replace_all(date, "February", "02")
,date = str_replace_all(date, "June", "06")
,date = str_replace_all(date, "Feb", "02")
,date = str_replace_all(date, "May", "05")
,date = str_replace_all(date, "Jun", "06")
,date = str_replace_all(date, "Thursday, ","")
,date = str_replace_all(date, ' ', '-')
,date = str_replace_all(date, "[A-z]",'')
,date = str_replace_all(date, '-----', '')
,date = str_replace_all(date, '----', '')
,date = str_replace_all(date, '---', '')
,date = str_replace_all(date, '--', '')
)
sona$date[36] <- "09-02-2023"
sona$year[36] <- "2023"
sona$date <- dmy(sona$date)
#| label: bag_of_words
sona <- sona %>%
select(date, speech, year, president) %>%
rename(President = president)
# Assign labels as integers
sona$prez_encoded <- as.integer(factor(sona$President))-1
unnest_reg <- "[^A-Za-z_\\d#@']"
replace_reg <- '(https?:.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;'
tidy_sentences <- sona %>%
mutate(speech, speech = str_replace_all(speech, "’", "'")) %>%
mutate(speech = str_replace_all(speech, replace_reg, '')) %>%
unnest_tokens(sentence, speech, token = 'sentences') %>%
filter(str_detect(sentence, '[a-z]')) %>%
mutate(sentID = row_number())
tidy_words<- tidy_sentences %>%
mutate(sentence, sentence = str_replace_all(sentence, "’", "'")) %>%
mutate(sentence = str_replace_all(sentence, replace_reg, '')) %>%
unnest_tokens(word, sentence, token = 'words') %>%
filter(str_detect(word, '[a-z]')) %>%
filter(!word %in% stop_words$word)
set.seed(1991)
speech_word_bag<- tidy_words %>%
count(word) %>%
top_n(200, wt = n) %>%
select(-n)
speech_tdf <- tidy_words %>%
inner_join(speech_word_bag) %>%
group_by(sentID, President, word) %>%
count() %>%
mutate(total = sum(n)) %>%
ungroup()
bag_of_words <- speech_tdf %>%
select(sentID, President, word, n) %>%
pivot_wider(names_from = word, values_from = n, values_fill = 0, names_repair = "unique")
# Assign labels as integers
bag_of_words$prez_encoded <- as.integer(factor(bag_of_words$President))-1
set.seed(1991)
sample_size <- floor(0.8 * nrow(bag_of_words))
train_indices <- sample(seq_len(nrow(bag_of_words)), size = sample_size)
train_data <-bag_of_words[train_indices, ]
test_data <- bag_of_words[-train_indices, ]
# Create a bag of words matrix for the training and test data
max_words <- ncol(bag_of_words)
x_train_bag<- as.matrix(train_data[, 3:(max_words-1)]) # Columns 3 and onwards are the BoW columns
x_test_bag <- as.matrix(test_data[, 3:(max_words-1)])
y_train_bag <- to_categorical(train_data$prez_encoded,
num_classes =length(unique(bag_of_words$prez_encoded)))
library(tensorflow)
remotes::install_github("rstudio/tensorflow")
remotes::install_github("rstudio/tensorflow", force=TRUE)
