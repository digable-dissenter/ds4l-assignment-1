,date = str_replace_all(date, "June", "06")
,date = str_replace_all(date, "Feb", "02")
,date = str_replace_all(date, "May", "05")
,date = str_replace_all(date, "Jun", "06")
,date = str_replace_all(date, "Thursday, ","")
,date = str_replace_all(date, ' ', '-')
,date = str_replace_all(date, "[A-z]",'')
,date = str_replace_all(date, '-----', '')
,date = str_replace_all(date, '----', '')
,date = str_replace_all(date, '---', '')
,date = str_replace_all(date, '--', '')
)
# Manually correct the date for the 36th speech
sona$date[36] <- "09-02-2023"
sona$year[36] <- "2023"
# Convert the date column to a date format
sona$date <- dmy(sona$date)
#| label: bag_of_words
#| message: false
# Select and rename columns for the bag-of-words representation
sona <- sona %>%
select(date, speech, year, president) %>%
rename(President = president)
# Assign labels as integers
sona$prez_encoded <- as.integer(factor(sona$President))-1
# Define a regular expression for text tokenization
unnest_reg <- "[^A-Za-z_\\d#@']"
# Tokenize the text into sentences
tidy_sentences <- sona %>%
mutate(speech, speech = str_replace_all(speech, "’", "'")) %>%
mutate(speech = str_replace_all(speech, replace_reg, '')) %>%
unnest_tokens(sentence, speech, token = 'sentences') %>%
filter(str_detect(sentence, '[a-z]')) %>%
mutate(sentID = row_number())
# Tokenize sentences into words
tidy_words<- tidy_sentences %>%
mutate(sentence, sentence = str_replace_all(sentence, "’", "'")) %>%
mutate(sentence = str_replace_all(sentence, replace_reg, '')) %>%
unnest_tokens(word, sentence, token = 'words') %>%
filter(str_detect(word, '[a-z]')) %>%
filter(!word %in% stop_words$word)
# Reproducibility
set.seed(1991)
# Create a bag-of-words representation
speech_word_bag <- tidy_words %>%
count(word) %>%
top_n(200, wt = n) %>%
select(-n) # Select the top 200 words based on word frequency
# Join the bag-of-words with the text data
speech_tdf <- tidy_words %>%
inner_join(speech_word_bag) %>%
group_by(sentID, President, word) %>%
count() %>%
mutate(total = sum(n)) %>%
ungroup()
# Create a wide format bag-of-words data frame
bag_of_words <- speech_tdf %>%
select(sentID, President, word, n) %>%
pivot_wider(names_from = word, values_from = n, values_fill = 0, names_repair = "unique")
#| label: feed_fwd_NN
# Assign labels as integers
bag_of_words$prez_encoded <- as.integer(factor(bag_of_words$President))-1
# Reproducibility
set.seed(1991)
# Define the sample size for the training data
sample_size <- floor(0.8 * nrow(bag_of_words))
# Randomly select indices for the training data
train_indices <- sample(seq_len(nrow(bag_of_words)), size = sample_size)
# Create training and test datasets based on the selected indices
train_data <-bag_of_words[train_indices, ]
test_data <- bag_of_words[-train_indices, ]
# Create bag-of-words matrices for the training and test data
max_words <- ncol(bag_of_words)
x_train_bag<- as.matrix(train_data[, 3:(max_words-1)]) # Extract the bag-of-words columns
x_test_bag <- as.matrix(test_data[, 3:(max_words-1)])
# Encode the labels (presidents) using one-hot encoding for the training and test data
y_train_bag <- to_categorical(train_data$prez_encoded,
num_classes =length(unique(bag_of_words$prez_encoded)))
y_test_bag <- to_categorical(test_data$prez_encoded,
num_classes =length(unique(bag_of_words$prez_encoded)))
# Reproducibility
set.seed(1991)
# Build a neural network model using the Keras library
model_bag <- keras_model_sequential()
model_bag %>%
layer_dense(units = 8, activation = 'relu', input_shape = c(max_words-3)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = max(train_data$prez_encoded) + 1, activation = 'softmax')
# Compile the model, specifying the loss function, optimizer, and evaluation metrics
model_bag %>%
compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = 'accuracy'
)
# Train the model on the training data
train_history_bag <- model_bag %>%
fit(
x_train_bag, y_train_bag,
epochs = 30,
validation_split = 0.2, shuffle = TRUE
)
# Plot the training history (loss and accuracy)
plot(train_history_bag)
# Evaluate the model on the test data
results_nn_bag <- model_bag %>%
evaluate(x_test_bag, y_test_bag, batch_size=128, verbose = 2)
results_nn_bag
#| label: word_embedding
# Select a subset of training data based on President labels
training_ids <- bag_of_words %>%
group_by(President) %>%
slice_sample(prop = 0.7) %>%
ungroup() %>%
select(sentID)
# Define the maximum number of features (words) to consider
max_features <- 10000
# Initialize a text tokenizer
tokenizer <- text_tokenizer(num_words = max_features)
# Fit the tokenizer on the sentences from tidy_sentences
fit_text_tokenizer(tokenizer, tidy_sentences$sentence)
# Convert text sequences to numerical sequences using the tokenizer
sequences <- tokenizer$texts_to_sequences(tidy_sentences$sentence)
# Encode the labels as integers
tidy_sentences$prez_encoded <- as.integer(factor(tidy_sentences$President))
# Convert labels to integers and subtract 1
y <- as.integer(tidy_sentences$prez_encoded)-1
# Identify training rows based on the selected training IDs
training_rows <- which(tidy_sentences$sentID %in% training_ids$sentID)
# Create training datasets
train_emb <- list()
train_emb$x <- sequences[training_rows]
train_emb$y <- y[training_rows]
train_emb$y <- to_categorical(train_emb$y,  num_classes = length(unique(tidy_sentences$prez_encoded)))
# Create test datasets
test_emb <- list()
test_emb$x <-  sequences[-training_rows]
test_emb$y <-  y[-training_rows]
test_emb$y<- to_categorical(test_emb$y,  num_classes = length(unique(tidy_sentences$prez_encoded)))
# Define the maximum sequence length
maxlen <- 32
# Pad the sequences to a fixed length
x_train_emb <- train_emb$x %>%
pad_sequences(maxlen = maxlen)
x_test_emb <- test_emb$x %>%
pad_sequences(maxlen = maxlen)
# Build a neural network model with word embeddings
model_emb <- keras_model_sequential() %>%
layer_embedding(max_features, output_dim = 10, input_length = maxlen) %>%
layer_dropout(0.2) %>%
layer_flatten() %>%
layer_dense(32, activation = "relu") %>%
layer_dense(units = 6, activation = "softmax")
# Compile the model, specifying loss, optimizer, and evaluation metric
model_emb %>% compile(
loss = "categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
# Train the model on the training data
train_history_emb <- model_emb %>%
fit(x_train_emb, x_train_emb$y, epochs = 10, verbose = 0)
x_train_emb
# Pad the sequences to a fixed length
x_train_emb <- train_emb$x %>%
pad_sequences(maxlen = maxlen)
x_test_emb <- test_emb$x %>%
pad_sequences(maxlen = maxlen)
# Create training datasets
train_emb <- list()
train_emb$x <- sequences[training_rows]
train_emb$y <- y[training_rows]
train_emb$y <- to_categorical(train_emb$y,  num_classes = length(unique(tidy_sentences$prez_encoded)))
# Train the model on the training data
train_history_emb <- model_emb %>%
fit(x_train_emb, train_emb$y, epochs = 10, verbose = 0)
# Convert the "prez_encoded" column in the training and test datasets to factors
train_data$prez_encoded <- as.factor(train_data$prez_encoded)
test_data$prez_encoded <- as.factor(test_data$prez_encoded)
# Define a grid of hyperparameters for Random Forest
ctrl <- trainControl(method = 'cv', number = 5, verboseIter = T)
# Grid of hyperparameter values for "mtry"
randomForest_grid <- expand.grid(mtry = c(10, 20, 30))
# Define the control settings for cross-validation
ctrl_gbm <- trainControl(method = 'cv', number = 5, verboseIter = T)
# Define a grid of hyperparameters for GBM
gbm_grid <- expand.grid(n.trees = c(100),
interaction.depth = c(1, 2, 6),
shrinkage = c(0.01, 0.005, 0.001),
n.minobsinnode = 1)
#| label: gbm
#load(file = "GradientBoosts.RData")
# Reproducibility
set.seed(1991)
# Define the control settings for cross-validation
ctrl_gbm <- trainControl(method = 'cv', number = 5, verboseIter = T)
# Define a grid of hyperparameters for GBM
gbm_grid <- expand.grid(n.trees = c(100),
interaction.depth = c(1, 2, 6),
shrinkage = c(0.01, 0.005, 0.001),
n.minobsinnode = 1)
# Perform a grid search for the GBM model
gbm_gridsearch <- train(fmla, data = data_to_train,
method = 'gbm',
trControl = ctrl_gbm,
verbose = F,
tuneGrid = gbm_grid)
fmla <- as.formula("prez_encoded ~ .")
data_to_train <- train_data[, -(1:2)]
# Perform a grid search for the GBM model
gbm_gridsearch <- train(fmla, data = data_to_train,
method = 'gbm',
trControl = ctrl_gbm,
verbose = F,
tuneGrid = gbm_grid)
??caret
??train
??train
# Perform a grid search for the GBM model
gbm_gridsearch <- caret::train(fmla, data = data_to_train,
method = 'gbm',
trControl = ctrl_gbm,
verbose = F,
tuneGrid = gbm_grid)
save(gbm_gridsearch, file = "GradientBoosts.RData")
# load(file = "GradientBoosts.RData")
# Plot the results of the grid search (parameter tuning)
plot(gbm_gridsearch)
## Prediction
# Make predictions on the test data using the selected GBM model
gbm_pred <- predict(gbm_gridsearch, test_data[,-(1:2)])
# Create a confusion matrix for evaluating the model's performance
confusionMatrix(gbm_pred, test_data$prez_encoded)
# Perform a grid search for the Random Forest model
randomForest_gridsearch <- caret::train(fmla, data = data_to_train,
method = 'rf',
trControl = ctrl_rf,
verbose = F,
tuneGrid = randomForest_grid)
# Define a grid of hyperparameters for Random Forest
ctrl_rf <- trainControl(method = 'cv', number = 5, verboseIter = T)
# Perform a grid search for the Random Forest model
randomForest_gridsearch <- caret::train(fmla, data = data_to_train,
method = 'rf',
trControl = ctrl_rf,
verbose = F,
tuneGrid = randomForest_grid)
save(gbm_gridsearch, file = "GradientBoosts.RData")
rf_pred <- predict(randomForest_gridsearch, newdata = test_data[,-(1:2)])
# Create a confusion matrix for evaluating the model's performance
confusionMatrix(rf_pred, test_data$prez_encoded) #First predicted, then truth
save(randomForest_gridsearch, file = 'Random Forest Model.RData')
save(gbm_gridsearch, file = "GradientBoosts.RData")
save(randomForest_gridsearch, file = 'RandomForests.RData')
# save(gbm_gridsearch, file = "GradientBoosts.RData")
# load(file = "GradientBoosts.RData")
# Plot the results of the grid search (parameter tuning)
plot(gbm_gridsearch)
## Prediction
# Make predictions on the test data using the selected GBM model
gbm_pred <- predict(gbm_gridsearch, test_data[,-(1:2)])
# Create a confusion matrix for evaluating the model's performance
confusionMatrix(gbm_pred, test_data$prez_encoded)
# Perform a grid search for the Random Forest model
#randomForest_gridsearch <- caret::train(fmla, data = data_to_train,
#                        method = 'rf',
#                        trControl = ctrl_rf,
#                        verbose = F,
#                        tuneGrid = randomForest_grid)
# The code above trains multiple Random Forest models with different "mtry"
plot(randomForest_gridsearch)
#| label: setup
#| include: false
knitr::opts_knit$set(root.dir = "C:/Users/User/OneDrive/Documents/School/2023/Masters/STA5073Z/Assignments/Assignment 1/ds4l-assignment-1/")
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
#| label: libraries
#| message: false
# Clear global environment
# rm(list=ls())
# Libraries we need
libs <- c('caret', 'dplyr', 'ggplot2', 'lubridate', 'keras', 'tm', 'quarto', 'readr', 'stringr', 'tensorflow', 'tidyr', 'tidytext', 'tfhub')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
use_virtualenv("r-tensorflow", required = TRUE)
#| label: feed_fwd_NN
#| message: false
# Assign labels as integers
bag_of_words$prez_encoded <- as.integer(factor(bag_of_words$President))-1
# Reproducibility
set.seed(1991)
# Define the sample size for the training data
sample_size <- floor(0.8 * nrow(bag_of_words))
# Randomly select indices for the training data
train_indices <- sample(seq_len(nrow(bag_of_words)), size = sample_size)
# Create training and test datasets based on the selected indices
train_data <-bag_of_words[train_indices, ]
test_data <- bag_of_words[-train_indices, ]
# Create bag-of-words matrices for the training and test data
max_words <- ncol(bag_of_words)
x_train_bag<- as.matrix(train_data[, 3:(max_words-1)]) # Extract the bag-of-words columns
x_test_bag <- as.matrix(test_data[, 3:(max_words-1)])
# Encode the labels (presidents) using one-hot encoding for the training and test data
y_train_bag <- to_categorical(train_data$prez_encoded,
num_classes =length(unique(bag_of_words$prez_encoded)))
y_test_bag <- to_categorical(test_data$prez_encoded,
num_classes =length(unique(bag_of_words$prez_encoded)))
# Reproducibility
set.seed(1991)
# Build a neural network model using the Keras library
model_bag <- keras_model_sequential()
model_bag %>%
layer_dense(units = 8, activation = 'relu', input_shape = c(max_words-3)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = max(train_data$prez_encoded) + 1, activation = 'softmax')
# Compile the model, specifying the loss function, optimizer, and evaluation metrics
model_bag %>%
compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = 'accuracy'
)
# Train the model on the training data
train_history_bag <- model_bag %>%
fit(
x_train_bag, y_train_bag,
epochs = 30,
validation_split = 0.2, shuffle = TRUE
)
# Plot the training history (loss and accuracy)
plot(train_history_bag)
# Evaluate the model on the test data
results_nn_bag <- model_bag %>%
evaluate(x_test_bag, y_test_bag, batch_size=128, verbose = 2)
results_nn_bag
results_nn_bag
results_nn_emb
results_cnn
#| label: cnn
#| message: false
# Create a sequential neural network model
model_cnn <- keras_model_sequential() %>%
# Add an embedding layer for word embeddings
layer_embedding(max_features, output_dim = 10, input_length = maxlen) %>%
# Apply dropout for regularization
layer_dropout(0.2) %>%
# Add a 1D convolutional layer with 64 filters, a kernel size of 8, and ReLU activation
layer_conv_1d(filters = 64, kernel_size = 8, activation = "relu") %>%
# Apply 1D max-pooling with a pool size of 2
layer_max_pooling_1d(pool_size = 2) %>%
# Flatten the data
layer_flatten() %>%
# Flatten the data
layer_dense(32, activation = "relu") %>%
# Output layer with 6 units and softmax activation for classification
layer_dense(units = 6, activation = "softmax")
# Compile the model, specifying loss, optimizer, and evaluation metric
model_cnn %>% compile(
loss = "categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
# Train the model on the training data
train_history_cnn <- model_cnn %>%
fit(x_train_emb, train_emb$y, batch_size = 64, epochs = 10, verbose = 0)
# Plot the training history (loss and accuracy)
plot(train_history_cnn)
# Evaluate the model on the test data
results_cnn <- model_cnn %>%
evaluate(x_test_emb, test_emb$y, batch_size = 64, verbose = 2)
results_cnn
model_bag %>%
layer_dense(units = 8, activation = 'relu', input_shape = c(max_words-3)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = max(train_data$prez_encoded) + 1, activation = 'softmax')
# Compile the model, specifying the loss function, optimizer, and evaluation metrics
model_bag %>%
compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = 'accuracy'
)
# Train the model on the training data
train_history_bag <- model_bag %>%
fit(
x_train_bag, y_train_bag,
epochs = 30,
validation_split = 0.2, shuffle = TRUE
)
??fit
#| label: setup
#| include: false
knitr::opts_knit$set(root.dir = "C:/Users/User/OneDrive/Documents/School/2023/Masters/STA5073Z/Assignments/Assignment 1/ds4l-assignment-1/")
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
#| label: libraries
#| message: false
# Clear global environment
# rm(list=ls())
# Libraries we need
libs <- c('caret', 'dplyr', 'ggplot2', 'lubridate', 'keras', 'tm', 'quarto', 'readr', 'stringr', 'tensorflow', 'tidyr', 'tidytext', 'tfhub')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
use_virtualenv("r-tensorflow", required = TRUE)
gbm_all <- list(gbm_gridsearch, gbm_pred, gbm_confusion)
# Create a confusion matrix for evaluating the model's performance
gbm_confusion <- confusionMatrix(gbm_pred, test_data$prez_encoded)
# Convert the "prez_encoded" column in the training and test datasets to factors
train_data$prez_encoded <- as.factor(train_data$prez_encoded)
test_data$prez_encoded <- as.factor(test_data$prez_encoded)
# Create a confusion matrix for evaluating the model's performance
gbm_confusion <- confusionMatrix(gbm_pred, test_data$prez_encoded)
gbm_all <- list(gbm_gridsearch, gbm_pred, gbm_confusion)
# Create a confusion matrix for evaluating the model's performance
rf_data <- confusionMatrix(rf_pred, test_data$prez_encoded) #First predicted, then truth
#|label: tbl-accuracy
#|ttbl-cap: Accuracy on test data in each Machine Learning models employed
model_accuracies<- round(c(results_nn_bag[[2]], results_nn_emb[[2]],
results_cnn[[2]],
rf_data[[3]]$overall[["Accuracy"]],
gbm_all[[3]]$overall[["Accuracy"]]
),4)
rf_data
rf_data[[3]]
rf_data[[3]]$overall
rf_data$overall
rf_data$overall[['Accuracy']]
tbl_model_acc
model_names <- c("Feed-Forward Neural Network (Bag of Words structure)",
"Feed-Forward Neural Network (Word Embeddings Words structure)",
"Convolutional Neural Nework (Word Embeddings structure)",
"Random Forest",
"Gradient Boost Machine")
tbl_model_acc <- cbind(model_names, model_accuracies)
model_accuracies<- round(c(results_nn_bag[[2]], results_nn_emb[[2]],
results_cnn[[2]],
rf_data$overall[["Accuracy"]],
gbm_all$overall[["Accuracy"]]
),4)
tbl_model_acc <- cbind(model_names, model_accuracies)
??colnames
colnames(tbl_model_acc) <- c("Model", "Test Accuracy")
rf_metrics <- list(randomForest_gridsearch, rf_pred, rf_conf)
# Create a confusion matrix for evaluating the model's performance
rf_conf <- confusionMatrix(rf_pred, test_data$prez_encoded) #First predicted, then truth
rf_metrics <- list(randomForest_gridsearch, rf_pred, rf_conf)
#|label: tbl-accuracy
#|ttbl-cap: Accuracy on test data in each Machine Learning models employed
model_accuracies<- round(c(results_nn_bag[[2]], results_nn_emb[[2]],
results_cnn[[2]],
rf_data$overall[["Accuracy"]],
gbm_all$overall[["Accuracy"]]
),4)
model_names <- c("Feed-Forward Neural Network (Bag of Words structure)",
"Feed-Forward Neural Network (Word Embeddings Words structure)",
"Convolutional Neural Nework (Word Embeddings structure)",
"Random Forest",
"Gradient Boost Machine")
tbl_model_acc <- cbind(model_names, model_accuracies)
colnames(tbl_model_acc) <- c("Model", "Test Accuracy")
kable(tbl_model_acc, format = "markdown", escape = FALSE, digits = 3, align = 'ccccc') %>%
kable_styling(bootstrap_options = "striped", full_width = TRUE)
#| label: libraries
#| message: false
# Clear global environment
# rm(list=ls())
# Libraries we need
libs <- c('caret', 'dplyr', 'ggplot2', 'lubridate', 'kable', 'keras', 'tm', 'quarto', 'readr', 'stringr', 'tensorflow', 'tidyr', 'tidytext', 'tfhub')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
#| label: tbl-clssaloc
#| tbl-cap: Class representation of presidents
prez <- c("De Klerk", "Mandela", "Mbeki", "Mothlane", "Zuma", "Ramaphosa")
clss <- c("Class 0", "Class 1","Class 2","Class 3","Class 4","Class 5")
kable(rbind(clss,prez), format = "markdown", escape = FALSE, digits = 3, align = 'cccccc') %>%
kable_styling(bootstrap_options = "striped", full_width = TRUE)
tbl_class_prez <- rbind(prez, clss)
kable(tbl_class_prez, format = "markdown", escape = FALSE, digits = 3, align = 'cccccc') %>%
kable_styling(bootstrap_options = "striped", full_width = TRUE)
kable(tbl_model_acc, format = "markdown", escape = FALSE, digits = 3, align = 'ccccc') %>%
kable_styling(bootstrap_options = "striped", full_width = TRUE)
#| label: libraries
#| message: false
# Clear global environment
# rm(list=ls())
# Libraries we need
libs <- c('caret', 'dplyr', 'ggplot2', 'lubridate', 'kableExtra', 'keras', 'tm', 'quarto', 'readr', 'stringr', 'tensorflow', 'tidyr', 'tidytext', 'tfhub')
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
use_virtualenv("r-tensorflow", required = TRUE)
kable(tbl_model_acc, format = "markdown", escape = FALSE, digits = 3, align = 'ccccc') %>%
kable_styling(bootstrap_options = "striped", full_width = TRUE)
kable(tbl_class_prez, format = "markdown", escape = FALSE, digits = 3, align = 'cccccc') %>%
kable_styling(bootstrap_options = "striped", full_width = TRUE)
